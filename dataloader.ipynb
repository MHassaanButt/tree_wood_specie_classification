{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb621511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 1: Imports & Global Configuration\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from operator import truediv\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# Paths\n",
    "PROCESSED_DIR = 'prepocessing/processed_cubes'\n",
    "FIG_DIR       = 'figures'\n",
    "RESULT_DIR    = 'results'\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "WS        = 8        # patch size\n",
    "NC        = 15        # number of spectral components\n",
    "DLM       = 'PCA'       # PCA dimensionality limit multiplier\n",
    "trRatio   = 0.05\n",
    "vrRatio   = 0.05     # note: tr+vr+te should sum to 1; teRatio computed below\n",
    "teRatio   = 0.90\n",
    "randomState = 345\n",
    "\n",
    "batch_size = 56\n",
    "epochs     = 50\n",
    "lr         = 1e-3\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce50cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 blocks from 9 species\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 2: Load Preprocessed Cubes & Labels\n",
    "\n",
    "def LoadHSIData(data_dir):\n",
    "    files = glob.glob(os.path.join(data_dir, \"*.npz\"))\n",
    "    species = sorted({os.path.basename(f).split('_block')[0] for f in files})\n",
    "    label_map = {sp:i for i,sp in enumerate(species)}\n",
    "    cubes, labels, names = [], [], []\n",
    "    for f in files:\n",
    "        sp = os.path.basename(f).split('_block')[0]\n",
    "        arr = np.load(f)['block']  # (H, W, C)\n",
    "        cubes.append(arr)\n",
    "        labels.append(label_map[sp])\n",
    "        names.append(sp)\n",
    "    return cubes, np.array(labels), names, species\n",
    "\n",
    "cubes, cube_labels, cube_names, species_list = LoadHSIData(PROCESSED_DIR)\n",
    "num_classes = len(species_list)\n",
    "print(f\"Loaded {len(cubes)} blocks from {num_classes} species\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89654da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# Cell 3: Plot Mean Spectral Signatures per Species\n",
    "\n",
    "mean_spectra = {sp:[] for sp in species_list}\n",
    "for arr, name in zip(cubes, cube_names):\n",
    "    mean_spectra[name].append(arr.mean(axis=(0,1)))  # mean spectrum\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for sp in species_list:\n",
    "    spec = np.stack(mean_spectra[sp], axis=0).mean(axis=0)\n",
    "    plt.plot(spec, label=sp)\n",
    "plt.title(\"Mean Spectral Signature by Species\")\n",
    "plt.xlabel(\"Band Index\")\n",
    "plt.ylabel(\"Mean Reflectance\")\n",
    "plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, \"mean_spectra.png\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c3cd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in: prepocessing/processed_cubes\n",
      "Found .npz files: ['prepocessing/processed_cubes/sapelimahonki_block0.npz', 'prepocessing/processed_cubes/afromasia_block0.npz', 'prepocessing/processed_cubes/tiiki_block1.npz', 'prepocessing/processed_cubes/ipe_block0.npz', 'prepocessing/processed_cubes/tiiki_block0.npz'] ... total: 26\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: make sure we have blocks to process\n",
    "print(\"Looking in:\", PROCESSED_DIR)\n",
    "files = glob.glob(os.path.join(PROCESSED_DIR, \"*.npz\"))\n",
    "print(\"Found .npz files:\", files[:5], \"... total:\", len(files))\n",
    "if len(files) == 0:\n",
    "    raise RuntimeError(f\"No .npz files in {PROCESSED_DIR}. Check your path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3919b0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated pixels: (1162087, 256)\n",
      "Fitted PCA: input bands → 15 components.\n",
      "Applied PCA to all cubes.\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 4 (Generalized): Global Dimensionality Reduction Fit & Apply\n",
    "\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA, SparsePCA, TruncatedSVD\n",
    "\n",
    "# 1) Aggregate all pixel spectra\n",
    "all_pixels = np.vstack([arr.reshape(-1, arr.shape[2]) for arr in cubes])\n",
    "print(\"Aggregated pixels:\", all_pixels.shape)\n",
    "\n",
    "# 2) Choose and fit the DR model once, based on DLM\n",
    "if DLM == 'PCA':\n",
    "    dr_model = PCA(n_components=NC, whiten=True, random_state=0)\n",
    "elif DLM == 'iPCA':\n",
    "    dr_model = IncrementalPCA(n_components=NC)\n",
    "elif DLM == 'KPCA':\n",
    "    dr_model = KernelPCA(kernel='rbf', n_components=NC,\n",
    "                        fit_inverse_transform=True, random_state=0)\n",
    "elif DLM == 'SPCA':\n",
    "    dr_model = SparsePCA(n_components=NC, alpha=1e-4, random_state=0)\n",
    "elif DLM == 'SVD':\n",
    "    dr_model = TruncatedSVD(n_components=NC, random_state=0)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown DLM: {DLM}\")\n",
    "\n",
    "# If iPCA, do a partial fit in batches\n",
    "if DLM == 'iPCA':\n",
    "    for batch in np.array_split(all_pixels, 256):\n",
    "        dr_model.partial_fit(batch)\n",
    "else:\n",
    "    dr_model.fit(all_pixels)\n",
    "\n",
    "# 3) Save the DR model\n",
    "joblib.dump(dr_model, os.path.join(RESULT_DIR, f\"{DLM}_model.joblib\"))\n",
    "print(f\"Fitted {DLM}: input bands → {NC} components.\")\n",
    "\n",
    "# 4) Apply the same transformation to each cube\n",
    "reduced_cubes = []\n",
    "for arr in cubes:\n",
    "    H, W, B = arr.shape\n",
    "    flat = arr.reshape(-1, B)\n",
    "    red  = dr_model.transform(flat)\n",
    "    reduced_cubes.append(red.reshape(H, W, NC))\n",
    "\n",
    "print(f\"Applied {DLM} to all cubes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db5040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1162087 patches of shape (8, 8, 15)\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 5: Create Spatial-Spectral Patches & Labels\n",
    "\n",
    "def ImageCubes(HSI_list, labels, WS=14):\n",
    "    patches, patch_labels = [], []\n",
    "    for HSI, lab in zip(HSI_list, labels):\n",
    "        H, W, NB = HSI.shape\n",
    "        pad = WS//2\n",
    "        padded = np.pad(HSI, ((pad,pad),(pad,pad),(0,0)), mode='constant')\n",
    "        for i in range(pad, pad+H):\n",
    "            for j in range(pad, pad+W):\n",
    "                cube = padded[i-pad:i+pad, j-pad:j+pad, :]\n",
    "                patches.append(cube)\n",
    "                patch_labels.append(lab)\n",
    "    patches = np.stack(patches, axis=0)              # (N, WS, WS, NC)\n",
    "    patch_labels = np.array(patch_labels)            # (N,)\n",
    "    return patches, patch_labels\n",
    "\n",
    "patches, patch_labels = ImageCubes(reduced_cubes, cube_labels, WS)\n",
    "print(f\"Created {patches.shape[0]} patches of shape {patches.shape[1:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea52a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits → Train: 58104, Val: 58104, Test: 1045879\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 6: Train/Val/Test Split & DataLoaders\n",
    "\n",
    "def TrTeSplit(X, y, trRatio, vrRatio, teRatio, rs=345):\n",
    "    X_trte, X_te, y_trte, y_te = train_test_split(X, y, test_size=teRatio,\n",
    "                                                  random_state=rs, stratify=y)\n",
    "    vr = vrRatio/(trRatio+vrRatio)\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(X_trte, y_trte, test_size=vr,\n",
    "                                              random_state=rs, stratify=y_trte)\n",
    "    return X_tr, X_va, X_te, y_tr, y_va, y_te\n",
    "\n",
    "X_tr, X_va, X_te, y_tr, y_va, y_te = TrTeSplit(patches, patch_labels,\n",
    "                                               trRatio, vrRatio, teRatio, randomState)\n",
    "\n",
    "# convert to tensors and dataloaders\n",
    "def to_loader(X, y, batch_size, shuffle=True):\n",
    "    # X: (N, WS, WS, NC) → (N, NC, WS, WS)\n",
    "    X = np.transpose(X, (0,3,1,2)).astype(np.float32)\n",
    "    ds = TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
    "\n",
    "train_dl = to_loader(X_tr, y_tr, batch_size)\n",
    "val_dl   = to_loader(X_va, y_va, batch_size, shuffle=False)\n",
    "test_dl  = to_loader(X_te, y_te, batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Splits → Train: {len(X_tr)}, Val: {len(X_va)}, Test: {len(X_te)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ff9780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hassaan/miniconda3/envs/seg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 7: 3D-CNN Definition\n",
    "\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes, in_bands):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1,16,(5,3,3),padding=(2,1,1)), nn.ReLU(),\n",
    "            nn.MaxPool3d((2,2,2)),\n",
    "            nn.Conv3d(16,32,(3,3,3),padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool3d((1,1,1))\n",
    "        )\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(1)  # B,1,NB,WS,WS\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = Simple3DCNN(num_classes=num_classes, in_bands=NC).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8311436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50  Train: loss=0.7765, acc=0.753  |  Val: loss=0.3244, acc=0.903\n",
      "Epoch 2/50  Train: loss=0.2471, acc=0.926  |  Val: loss=0.2826, acc=0.909\n",
      "Epoch 3/50  Train: loss=0.1855, acc=0.943  |  Val: loss=0.1789, acc=0.945\n",
      "Epoch 4/50  Train: loss=0.1533, acc=0.953  |  Val: loss=0.1335, acc=0.958\n",
      "Epoch 5/50  Train: loss=0.1397, acc=0.957  |  Val: loss=0.1227, acc=0.963\n",
      "Epoch 6/50  Train: loss=0.1239, acc=0.961  |  Val: loss=0.1334, acc=0.958\n",
      "Epoch 7/50  Train: loss=0.1145, acc=0.964  |  Val: loss=0.1297, acc=0.959\n",
      "Epoch 8/50  Train: loss=0.1084, acc=0.965  |  Val: loss=0.1230, acc=0.963\n",
      "Epoch 9/50  Train: loss=0.1024, acc=0.967  |  Val: loss=0.0950, acc=0.970\n",
      "Epoch 10/50  Train: loss=0.0968, acc=0.969  |  Val: loss=0.0890, acc=0.971\n",
      "Epoch 11/50  Train: loss=0.0904, acc=0.971  |  Val: loss=0.0811, acc=0.975\n",
      "Epoch 12/50  Train: loss=0.0882, acc=0.971  |  Val: loss=0.1057, acc=0.967\n",
      "Epoch 13/50  Train: loss=0.0829, acc=0.973  |  Val: loss=0.0964, acc=0.969\n",
      "Epoch 14/50  Train: loss=0.0808, acc=0.973  |  Val: loss=0.0870, acc=0.972\n",
      "Epoch 15/50  Train: loss=0.0769, acc=0.975  |  Val: loss=0.0785, acc=0.975\n",
      "Epoch 16/50  Train: loss=0.0737, acc=0.976  |  Val: loss=0.0676, acc=0.978\n",
      "Epoch 17/50  Train: loss=0.0748, acc=0.975  |  Val: loss=0.0680, acc=0.979\n",
      "Epoch 18/50  Train: loss=0.0692, acc=0.977  |  Val: loss=0.1090, acc=0.964\n",
      "Epoch 19/50  Train: loss=0.0673, acc=0.978  |  Val: loss=0.0726, acc=0.976\n",
      "Epoch 20/50  Train: loss=0.0661, acc=0.978  |  Val: loss=0.0730, acc=0.977\n",
      "Epoch 21/50  Train: loss=0.0645, acc=0.979  |  Val: loss=0.0675, acc=0.977\n",
      "Epoch 22/50  Train: loss=0.0642, acc=0.979  |  Val: loss=0.0656, acc=0.978\n",
      "Epoch 23/50  Train: loss=0.0621, acc=0.980  |  Val: loss=0.0690, acc=0.978\n",
      "Epoch 24/50  Train: loss=0.0586, acc=0.980  |  Val: loss=0.0657, acc=0.980\n",
      "Epoch 25/50  Train: loss=0.0597, acc=0.980  |  Val: loss=0.0701, acc=0.977\n",
      "Epoch 26/50  Train: loss=0.0572, acc=0.981  |  Val: loss=0.0657, acc=0.978\n",
      "Epoch 27/50  Train: loss=0.0580, acc=0.980  |  Val: loss=0.0596, acc=0.980\n",
      "Epoch 28/50  Train: loss=0.0538, acc=0.982  |  Val: loss=0.0573, acc=0.980\n",
      "Epoch 29/50  Train: loss=0.0534, acc=0.982  |  Val: loss=0.0793, acc=0.975\n",
      "Epoch 30/50  Train: loss=0.0525, acc=0.982  |  Val: loss=0.0665, acc=0.978\n",
      "Epoch 31/50  Train: loss=0.0510, acc=0.983  |  Val: loss=0.0550, acc=0.981\n",
      "Epoch 32/50  Train: loss=0.0516, acc=0.983  |  Val: loss=0.0575, acc=0.980\n",
      "Epoch 33/50  Train: loss=0.0512, acc=0.983  |  Val: loss=0.0572, acc=0.981\n",
      "Epoch 34/50  Train: loss=0.0494, acc=0.983  |  Val: loss=0.0485, acc=0.983\n",
      "Epoch 35/50  Train: loss=0.0470, acc=0.984  |  Val: loss=0.0648, acc=0.977\n",
      "Epoch 36/50  Train: loss=0.0468, acc=0.984  |  Val: loss=0.0456, acc=0.984\n",
      "Epoch 37/50  Train: loss=0.0449, acc=0.984  |  Val: loss=0.0481, acc=0.983\n",
      "Epoch 38/50  Train: loss=0.0466, acc=0.984  |  Val: loss=0.0492, acc=0.983\n",
      "Epoch 39/50  Train: loss=0.0435, acc=0.985  |  Val: loss=0.0530, acc=0.982\n",
      "Epoch 40/50  Train: loss=0.0457, acc=0.984  |  Val: loss=0.0482, acc=0.983\n",
      "Epoch 41/50  Train: loss=0.0440, acc=0.985  |  Val: loss=0.0517, acc=0.982\n",
      "Epoch 42/50  Train: loss=0.0451, acc=0.985  |  Val: loss=0.0491, acc=0.982\n",
      "Epoch 43/50  Train: loss=0.0423, acc=0.985  |  Val: loss=0.0483, acc=0.983\n",
      "Epoch 44/50  Train: loss=0.0405, acc=0.986  |  Val: loss=0.0603, acc=0.981\n",
      "Epoch 45/50  Train: loss=0.0439, acc=0.985  |  Val: loss=0.0482, acc=0.983\n",
      "Epoch 46/50  Train: loss=0.0400, acc=0.986  |  Val: loss=0.0518, acc=0.983\n",
      "Epoch 47/50  Train: loss=0.0410, acc=0.986  |  Val: loss=0.0628, acc=0.980\n",
      "Epoch 48/50  Train: loss=0.0407, acc=0.986  |  Val: loss=0.0539, acc=0.982\n",
      "Epoch 49/50  Train: loss=0.0393, acc=0.987  |  Val: loss=0.0458, acc=0.985\n",
      "Epoch 50/50  Train: loss=0.0373, acc=0.987  |  Val: loss=0.0492, acc=0.983\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 8: Training Loop with Metrics Tracking\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs,  val_accs   = [], []\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    # training\n",
    "    model.train()\n",
    "    tloss, tcorrect, ttotal = 0, 0, 0\n",
    "    for x,y in train_dl:\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        loss   = F.cross_entropy(logits, y)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        preds = logits.argmax(1)\n",
    "        tloss   += loss.item()*y.size(0)\n",
    "        tcorrect+= (preds==y).sum().item()\n",
    "        ttotal  += y.size(0)\n",
    "    train_losses.append(tloss/ttotal)\n",
    "    train_accs.append(tcorrect/ttotal)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    vloss, vcorrect, vtotal = 0,0,0\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_dl:\n",
    "            x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "            logits = model(x)\n",
    "            loss  = F.cross_entropy(logits, y)\n",
    "            preds = logits.argmax(1)\n",
    "            vloss   += loss.item()*y.size(0)\n",
    "            vcorrect+= (preds==y).sum().item()\n",
    "            vtotal  += y.size(0)\n",
    "    val_losses.append(vloss/vtotal)\n",
    "    val_accs.append(vcorrect/vtotal)\n",
    "\n",
    "    print(f\"Epoch {ep}/{epochs}  Train: loss={train_losses[-1]:.4f}, acc={train_accs[-1]:.3f}  |  \"\n",
    "          f\"Val: loss={val_losses[-1]:.4f}, acc={val_accs[-1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "453a5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# Cell 9: Plot Loss and Accuracy Curves\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,5))\n",
    "axs[0].plot(train_losses, label='Train'); axs[0].plot(val_losses, label='Val')\n",
    "axs[0].set_title(\"Loss\"); axs[0].legend()\n",
    "axs[1].plot(train_accs, label='Train'); axs[1].plot(val_accs, label='Val')\n",
    "axs[1].set_title(\"Accuracy\"); axs[1].legend()\n",
    "for ax in axs: ax.set_xlabel(\"Epoch\")\n",
    "plt.savefig(os.path.join(FIG_DIR, \"train_val_curves.png\"))\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74dba7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "       abachi       0.95      0.99      0.97    132565\n",
      "    afromasia       0.98      1.00      0.99     83214\n",
      "          ipe       0.99      0.95      0.97    129307\n",
      "        iroko       1.00      1.00      1.00     86546\n",
      "       merbau       0.99      0.99      0.99    119147\n",
      "      ovangol       0.98      0.99      0.99    118237\n",
      "       padauk       0.99      0.99      0.99    119854\n",
      "sapelimahonki       0.98      0.99      0.98    116465\n",
      "        tiiki       0.99      0.96      0.97    140544\n",
      "\n",
      "     accuracy                           0.98   1045879\n",
      "    macro avg       0.98      0.98      0.98   1045879\n",
      " weighted avg       0.98      0.98      0.98   1045879\n",
      "\n",
      "Overall Accuracy: 0.983\n",
      "Cohen's Kappa: 0.981\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 10: Final Evaluation & Confusion Matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x,y in test_dl:\n",
    "        x = x.to(DEVICE)\n",
    "        p = model(x).argmax(1).cpu().numpy()\n",
    "        all_preds.append(p)\n",
    "        all_labels.append(y.numpy())\n",
    "all_preds  = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(all_labels, all_preds, target_names=species_list)\n",
    "cm     = confusion_matrix(all_labels, all_preds)\n",
    "kappa  = cohen_kappa_score(all_labels, all_preds)\n",
    "\n",
    "print(report)\n",
    "print(f\"Overall Accuracy: {accuracy_score(all_labels, all_preds):.3f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.3f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=species_list, yticklabels=species_list)\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\");\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, \"confusion_matrix.png\"))\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
