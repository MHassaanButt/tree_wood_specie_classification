{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb621511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 1: Imports & Global Configuration\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from operator import truediv\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# Paths\n",
    "PROCESSED_DIR = 'prepocessing/processed_cubes'\n",
    "FIG_DIR       = 'figures'\n",
    "RESULT_DIR    = 'results'\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "WS        = 14        # patch size\n",
    "NC        = 5        # number of spectral components\n",
    "DLM       = 'PCA'       # PCA dimensionality limit multiplier\n",
    "trRatio   = 0.90\n",
    "vrRatio   = 0.05     # note: tr+vr+te should sum to 1; teRatio computed below\n",
    "teRatio   = 0.05\n",
    "randomState = 345\n",
    "\n",
    "batch_size = 56\n",
    "epochs     = 50\n",
    "lr         = 1e-3\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ce50cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 blocks from 9 species\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 2: Load Preprocessed Cubes & Labels\n",
    "\n",
    "def LoadHSIData(data_dir):\n",
    "    files = glob.glob(os.path.join(data_dir, \"*.npz\"))\n",
    "    species = sorted({os.path.basename(f).split('_block')[0] for f in files})\n",
    "    label_map = {sp:i for i,sp in enumerate(species)}\n",
    "    cubes, labels, names = [], [], []\n",
    "    for f in files:\n",
    "        sp = os.path.basename(f).split('_block')[0]\n",
    "        arr = np.load(f)['block']  # (H, W, C)\n",
    "        cubes.append(arr)\n",
    "        labels.append(label_map[sp])\n",
    "        names.append(sp)\n",
    "    return cubes, np.array(labels), names, species\n",
    "\n",
    "cubes, cube_labels, cube_names, species_list = LoadHSIData(PROCESSED_DIR)\n",
    "num_classes = len(species_list)\n",
    "print(f\"Loaded {len(cubes)} blocks from {num_classes} species\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89654da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# Cell 3: Plot Mean Spectral Signatures per Species\n",
    "\n",
    "mean_spectra = {sp:[] for sp in species_list}\n",
    "for arr, name in zip(cubes, cube_names):\n",
    "    mean_spectra[name].append(arr.mean(axis=(0,1)))  # mean spectrum\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for sp in species_list:\n",
    "    spec = np.stack(mean_spectra[sp], axis=0).mean(axis=0)\n",
    "    plt.plot(spec, label=sp)\n",
    "plt.title(\"Mean Spectral Signature by Species\")\n",
    "plt.xlabel(\"Band Index\")\n",
    "plt.ylabel(\"Mean Reflectance\")\n",
    "plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, \"mean_spectra.png\"))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c3cd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in: prepocessing/processed_cubes\n",
      "Found .npz files: ['prepocessing/processed_cubes/sapelimahonki_block2.npz', 'prepocessing/processed_cubes/ipe_block1.npz', 'prepocessing/processed_cubes/abachi_block1.npz', 'prepocessing/processed_cubes/afromasia_block0.npz', 'prepocessing/processed_cubes/sapelimahonki_block0.npz'] ... total: 26\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: make sure we have blocks to process\n",
    "print(\"Looking in:\", PROCESSED_DIR)\n",
    "files = glob.glob(os.path.join(PROCESSED_DIR, \"*.npz\"))\n",
    "print(\"Found .npz files:\", files[:5], \"... total:\", len(files))\n",
    "if len(files) == 0:\n",
    "    raise RuntimeError(f\"No .npz files in {PROCESSED_DIR}. Check your path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3919b0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated pixels: (1162087, 256)\n",
      "Fitted PCA: input bands → 5 components.\n",
      "Applied PCA to all cubes.\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 4 (Generalized): Global Dimensionality Reduction Fit & Apply\n",
    "\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA, SparsePCA, TruncatedSVD\n",
    "\n",
    "# 1) Aggregate all pixel spectra\n",
    "all_pixels = np.vstack([arr.reshape(-1, arr.shape[2]) for arr in cubes])\n",
    "print(\"Aggregated pixels:\", all_pixels.shape)\n",
    "\n",
    "# 2) Choose and fit the DR model once, based on DLM\n",
    "if DLM == 'PCA':\n",
    "    dr_model = PCA(n_components=NC, whiten=True, random_state=0)\n",
    "elif DLM == 'iPCA':\n",
    "    dr_model = IncrementalPCA(n_components=NC)\n",
    "elif DLM == 'KPCA':\n",
    "    dr_model = KernelPCA(kernel='rbf', n_components=NC,\n",
    "                        fit_inverse_transform=True, random_state=0)\n",
    "elif DLM == 'SPCA':\n",
    "    dr_model = SparsePCA(n_components=NC, alpha=1e-4, random_state=0)\n",
    "elif DLM == 'SVD':\n",
    "    dr_model = TruncatedSVD(n_components=NC, random_state=0)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown DLM: {DLM}\")\n",
    "\n",
    "# If iPCA, do a partial fit in batches\n",
    "if DLM == 'iPCA':\n",
    "    for batch in np.array_split(all_pixels, 256):\n",
    "        dr_model.partial_fit(batch)\n",
    "else:\n",
    "    dr_model.fit(all_pixels)\n",
    "\n",
    "# 3) Save the DR model\n",
    "joblib.dump(dr_model, os.path.join(RESULT_DIR, f\"{DLM}_model.joblib\"))\n",
    "print(f\"Fitted {DLM}: input bands → {NC} components.\")\n",
    "\n",
    "# 4) Apply the same transformation to each cube\n",
    "reduced_cubes = []\n",
    "for arr in cubes:\n",
    "    H, W, B = arr.shape\n",
    "    flat = arr.reshape(-1, B)\n",
    "    red  = dr_model.transform(flat)\n",
    "    reduced_cubes.append(red.reshape(H, W, NC))\n",
    "\n",
    "print(f\"Applied {DLM} to all cubes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4db5040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1162087 patches of shape (14, 14, 5)\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 5: Create Spatial-Spectral Patches & Labels\n",
    "\n",
    "def ImageCubes(HSI_list, labels, WS=14):\n",
    "    patches, patch_labels = [], []\n",
    "    for HSI, lab in zip(HSI_list, labels):\n",
    "        H, W, NB = HSI.shape\n",
    "        pad = WS//2\n",
    "        padded = np.pad(HSI, ((pad,pad),(pad,pad),(0,0)), mode='constant')\n",
    "        for i in range(pad, pad+H):\n",
    "            for j in range(pad, pad+W):\n",
    "                cube = padded[i-pad:i+pad, j-pad:j+pad, :]\n",
    "                patches.append(cube)\n",
    "                patch_labels.append(lab)\n",
    "    patches = np.stack(patches, axis=0)              # (N, WS, WS, NC)\n",
    "    patch_labels = np.array(patch_labels)            # (N,)\n",
    "    return patches, patch_labels\n",
    "\n",
    "patches, patch_labels = ImageCubes(reduced_cubes, cube_labels, WS)\n",
    "print(f\"Created {patches.shape[0]} patches of shape {patches.shape[1:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea52a17",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 6: Train/Val/Test Split & DataLoaders\n",
    "\n",
    "def TrTeSplit(X, y, trRatio, vrRatio, teRatio, rs=345):\n",
    "    X_trte, X_te, y_trte, y_te = train_test_split(X, y, test_size=teRatio,\n",
    "                                                  random_state=rs, stratify=y)\n",
    "    vr = vrRatio/(trRatio+vrRatio)\n",
    "    X_tr, X_va, y_tr, y_va = train_test_split(X_trte, y_trte, test_size=vr,\n",
    "                                              random_state=rs, stratify=y_trte)\n",
    "    return X_tr, X_va, X_te, y_tr, y_va, y_te\n",
    "\n",
    "X_tr, X_va, X_te, y_tr, y_va, y_te = TrTeSplit(patches, patch_labels,\n",
    "                                               trRatio, vrRatio, teRatio, randomState)\n",
    "\n",
    "# convert to tensors and dataloaders\n",
    "def to_loader(X, y, batch_size, shuffle=True):\n",
    "    # X: (N, WS, WS, NC) → (N, NC, WS, WS)\n",
    "    X = np.transpose(X, (0,3,1,2)).astype(np.float32)\n",
    "    ds = TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
    "\n",
    "train_dl = to_loader(X_tr, y_tr, batch_size)\n",
    "val_dl   = to_loader(X_va, y_va, batch_size, shuffle=False)\n",
    "test_dl  = to_loader(X_te, y_te, batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Splits → Train: {len(X_tr)}, Val: {len(X_va)}, Test: {len(X_te)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff9780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       abachi       0.75      1.00      0.86         3\n",
      "    afromasia       0.00      0.00      0.00         2\n",
      "          ipe       1.00      0.33      0.50         3\n",
      "        iroko       0.43      1.00      0.60         3\n",
      "       merbau       1.00      0.67      0.80         3\n",
      "      ovangol       0.50      1.00      0.67         3\n",
      "       padauk       0.75      1.00      0.86         3\n",
      "sapelimahonki       0.50      0.33      0.40         3\n",
      "        tiiki       0.00      0.00      0.00         3\n",
      "\n",
      "     accuracy                           0.62        26\n",
      "    macro avg       0.55      0.59      0.52        26\n",
      " weighted avg       0.57      0.62      0.54        26\n",
      "\n",
      "Overall Accuracy: 0.6153846153846154\n",
      "Cohen's Kappa: 0.5652173913043479\n",
      "Confusion Matrix:\n",
      " [[3 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 1 0 0]\n",
      " [0 0 1 1 0 1 0 0 0]\n",
      " [0 0 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 2 1 0 0 0]\n",
      " [0 0 0 0 0 3 0 0 0]\n",
      " [0 0 0 0 0 0 3 0 0]\n",
      " [1 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 1 0 1 0 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/users/mubut0522/miniconda3/envs/hsi_torch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/mnt/users/mubut0522/miniconda3/envs/hsi_torch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/mnt/users/mubut0522/miniconda3/envs/hsi_torch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 7: 3D-CNN Definition\n",
    "\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes, in_bands):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1,16,(5,3,3),padding=(2,1,1)), nn.ReLU(),\n",
    "            nn.MaxPool3d((2,2,2)),\n",
    "            nn.Conv3d(16,32,(3,3,3),padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool3d((1,1,1))\n",
    "        )\n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(1)  # B,1,NB,WS,WS\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = Simple3DCNN(num_classes=num_classes, in_bands=NC).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# Cell 8: Training Loop with Metrics Tracking\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs,  val_accs   = [], []\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    # training\n",
    "    model.train()\n",
    "    tloss, tcorrect, ttotal = 0, 0, 0\n",
    "    for x,y in train_dl:\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        loss   = F.cross_entropy(logits, y)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        preds = logits.argmax(1)\n",
    "        tloss   += loss.item()*y.size(0)\n",
    "        tcorrect+= (preds==y).sum().item()\n",
    "        ttotal  += y.size(0)\n",
    "    train_losses.append(tloss/ttotal)\n",
    "    train_accs.append(tcorrect/ttotal)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    vloss, vcorrect, vtotal = 0,0,0\n",
    "    with torch.no_grad():\n",
    "        for x,y in val_dl:\n",
    "            x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "            logits = model(x)\n",
    "            loss  = F.cross_entropy(logits, y)\n",
    "            preds = logits.argmax(1)\n",
    "            vloss   += loss.item()*y.size(0)\n",
    "            vcorrect+= (preds==y).sum().item()\n",
    "            vtotal  += y.size(0)\n",
    "    val_losses.append(vloss/vtotal)\n",
    "    val_accs.append(vcorrect/vtotal)\n",
    "\n",
    "    print(f\"Epoch {ep}/{epochs}  Train: loss={train_losses[-1]:.4f}, acc={train_accs[-1]:.3f}  |  \"\n",
    "          f\"Val: loss={val_losses[-1]:.4f}, acc={val_accs[-1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# Cell 9: Plot Loss and Accuracy Curves\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,5))\n",
    "axs[0].plot(train_losses, label='Train'); axs[0].plot(val_losses, label='Val')\n",
    "axs[0].set_title(\"Loss\"); axs[0].legend()\n",
    "axs[1].plot(train_accs, label='Train'); axs[1].plot(val_accs, label='Val')\n",
    "axs[1].set_title(\"Accuracy\"); axs[1].legend()\n",
    "for ax in axs: ax.set_xlabel(\"Epoch\")\n",
    "plt.savefig(os.path.join(FIG_DIR, \"train_val_curves.png\"))\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dba7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# Cell 10: Final Evaluation & Confusion Matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x,y in test_dl:\n",
    "        x = x.to(DEVICE)\n",
    "        p = model(x).argmax(1).cpu().numpy()\n",
    "        all_preds.append(p)\n",
    "        all_labels.append(y.numpy())\n",
    "all_preds  = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(all_labels, all_preds, target_names=species_list)\n",
    "cm     = confusion_matrix(all_labels, all_preds)\n",
    "kappa  = cohen_kappa_score(all_labels, all_preds)\n",
    "\n",
    "print(report)\n",
    "print(f\"Overall Accuracy: {accuracy_score(all_labels, all_preds):.3f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.3f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=species_list, yticklabels=species_list)\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\");\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG_DIR, \"confusion_matrix.png\"))\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsi_torch",
   "language": "python",
   "name": "hsi_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
