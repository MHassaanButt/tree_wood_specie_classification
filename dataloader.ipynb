{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb621511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 1: Imports & Paths\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "\n",
    "# Point this at your folder of trimmed .npz cubes:\n",
    "PROCESSED_DIR = 'prepocessing/processed_cubes/'\n",
    "\n",
    "# List your species and build a label map (must match your saved filenames)\n",
    "species_list = ['abachi','afromasia','ipe','iroko','merbau','ovangol','padauk','sapelimahonki','tiiki']\n",
    "label_map    = {sp:i for i,sp in enumerate(species_list)}\n",
    "\n",
    "# Hyperparams\n",
    "BATCH_SIZE = 8\n",
    "LR         = 1e-3\n",
    "EPOCHS     = 50\n",
    "DEVICE     = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce50cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Center‐crop size: 263×88\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 2: Determine a Uniform Crop Size\n",
    "\n",
    "# Scan all .npz files to find the minimum H and W\n",
    "files   = glob.glob(os.path.join(PROCESSED_DIR, \"*.npz\"))\n",
    "heights = []\n",
    "widths  = []\n",
    "for f in files:\n",
    "    arr = np.load(f)['block']   # shape (H, W, C)\n",
    "    h, w, _ = arr.shape\n",
    "    heights.append(h)\n",
    "    widths.append(w)\n",
    "\n",
    "crop_h = min(heights)\n",
    "crop_w = min(widths)\n",
    "print(f\"→ Center‐crop size: {crop_h}×{crop_w}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89654da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 26\n",
      "One sample: torch.Size([15, 263, 88]) (bands×H×W), label=7\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 3: Dataset & DataLoader with PCA Reduction to 15 Bands\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the PCA model you fitted earlier\n",
    "pca = joblib.load(\"pca_15bands.joblib\")\n",
    "N_BANDS = pca.n_components_  # should be 15\n",
    "\n",
    "class ProcessedHSIDatasetPCA(Dataset):\n",
    "    def __init__(self, data_dir, label_map, crop_h, crop_w):\n",
    "        self.files     = glob.glob(os.path.join(data_dir, \"*.npz\"))\n",
    "        self.label_map = label_map\n",
    "        self.crop_h    = crop_h\n",
    "        self.crop_w    = crop_w\n",
    "        self.pca       = pca\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path  = self.files[idx]\n",
    "        fname = os.path.basename(path)\n",
    "        sp    = fname.split('_block')[0]\n",
    "        y     = self.label_map[sp]\n",
    "\n",
    "        block = np.load(path)['block']      # (H, W, C_orig)\n",
    "        H, W, C = block.shape\n",
    "\n",
    "        # 1) center-crop to (crop_h, crop_w)\n",
    "        top  = (H - self.crop_h)//2\n",
    "        left = (W - self.crop_w)//2\n",
    "        crop = block[top:top+self.crop_h, left:left+self.crop_w, :]\n",
    "\n",
    "        # 2) flatten & PCA → (crop_h*crop_w, C_orig) → transform → reshape\n",
    "        flat    = crop.reshape(-1, C)\n",
    "        reduced = self.pca.transform(flat)  # (crop_h*crop_w, N_BANDS)\n",
    "        patch   = reduced.reshape(self.crop_h, self.crop_w, N_BANDS)\n",
    "\n",
    "        # 3) to tensor (C, H, W)\n",
    "        patch = np.transpose(patch, (2,0,1)).astype(np.float32)\n",
    "        return torch.from_numpy(patch), torch.tensor(y)\n",
    "\n",
    "# Instantiate PCA dataset\n",
    "ds = ProcessedHSIDatasetPCA(PROCESSED_DIR, label_map, crop_h, crop_w)\n",
    "dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"Dataset size: {len(ds)}\")\n",
    "sample, label = ds[0]\n",
    "print(f\"One sample: {sample.shape} (bands×H×W), label={label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect which labels actually occur in your dataset\n",
    "all_labels = [ ds[i][1].item() for i in range(len(ds)) ]\n",
    "print(\"Unique labels in data:\", sorted(set(all_labels)))\n",
    "print(\"Total classes defined:\", len(label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# Cell 4: Simple 3D‐CNN Definition\n",
    "\n",
    "class Simple3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes, in_bands):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # input: (B, 1, in_bands, H, W)\n",
    "            nn.Conv3d(1, 16, kernel_size=(5,3,3), padding=(2,1,1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d((2,2,2)),\n",
    "            nn.Conv3d(16,32,(3,3,3), padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool3d((1,1,1)),\n",
    "        )\n",
    "        self.classifier = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)           # (B,1,Bands,H,W)\n",
    "        x = self.features(x)         # (B,32,1,1,1)\n",
    "        x = x.view(x.size(0), -1)    # (B,32)\n",
    "        return self.classifier(x)    # (B,num_classes)\n",
    "\n",
    "# infer number of bands from first sample\n",
    "sample, _ = ds[0]\n",
    "bands = sample.shape[0]\n",
    "model = Simple3DCNN(num_classes=len(species_list), in_bands=bands).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea52a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 — Loss: 2.2438, Acc: 0.038\n"
     ]
    }
   ],
   "source": [
    "# %%  \n",
    "# Cell 5: Training Loop & Validation Check\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss, running_correct, running_total = 0, 0, 0\n",
    "\n",
    "    for x, y in dl:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        loss   = F.cross_entropy(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(1)\n",
    "        running_loss   += loss.item() * y.size(0)\n",
    "        running_correct+= (preds==y).sum().item()\n",
    "        running_total  += y.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / running_total\n",
    "    epoch_acc  = running_correct / running_total\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} — Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  \n",
    "# Cell 6: Final Metrics\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x,y in dl:\n",
    "        x = x.to(DEVICE)\n",
    "        p = model(x).argmax(1).cpu().numpy()\n",
    "        all_preds.append(p)\n",
    "        all_labels.append(y.numpy())\n",
    "\n",
    "all_preds  = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=species_list))\n",
    "print(\"Overall Accuracy:\", (all_preds==all_labels).mean())\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(all_labels, all_preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hsi_torch",
   "language": "python",
   "name": "hsi_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
